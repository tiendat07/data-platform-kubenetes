apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: gpu-sample-2
  namespace: default
spec:
  type: Python
  mode: cluster
  image: docker.io/tiendat2807/spark-gpu:v0.4.0
  imagePullPolicy: Always
  sparkVersion: 3.5.6
  mainApplicationFile: local:///opt/spark/examples/src/main/python/pi.py
  restartPolicy:
    type: Never
  volumes:
    - name: "test-volume"
      hostPath:
        path: "/tmp"
        type: Directory

  sparkConf:
    "spark.kubernetes.container.image": "docker.io/tiendat2807/spark-gpu:v0.4.0"
    "spark.ui.port": "4045"
    "spark.rapids.sql.concurrentGpuTasks": "1"
    "spark.executor.resource.gpu.amount": "1"
    "spark.task.resource.gpu.amount": "1"
    "spark.executor.memory": "1g"
    "spark.rapids.memory.pinnedPool.size": "256m"
    "spark.executor.memoryOverhead": "512m"
    "spark.sql.files.maxPartitionBytes": "512m"
    "spark.sql.shuffle.partitions": "10"
    "spark.plugins": "com.nvidia.spark.SQLPlugin"
    "spark.executor.resource.gpu.discoveryScript": "/opt/sparkRapidsPlugin/getGpusResources.sh"
    "spark.executor.resource.gpu.vendor": "nvidia.com"
    "spark.executor.extraClassPath": "/opt/sparkRapidsPlugin/rapids-4-spark_2.12-25.06.0.jar"
    "spark.driver.extraClassPath": "/opt/sparkRapidsPlugin/rapids-4-spark_2.12-25.06.0.jar"

  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "1024m"
    labels:
      version: 3.5.6
    serviceAccount: spark
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"
  executor:
    annotations:
      sparkoperator.k8s.io/runtime-class: nvidia
    cores: 1
    instances: 1
    memory: "1024m"
    nodeSelector:                         # same selector you used
      accelerator: nvidia
      kubernetes.io/hostname: dao-tien-dat
    gpu:                                  # nvidia.com/gpu: 1
      name: nvidia.com/gpu
      quantity: 1
    labels:
      version: 3.5.6
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"